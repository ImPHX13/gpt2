This repository contains the implementation of the GPT-2 (124M) language model, designed to replicate and explore the capabilities of one of the most advanced natural language processing models developed by OpenAI. This implementation focuses on creating a clear and accessible framework and is inspired by Andrej Karpathy's nanogpt.

If you lack a local GPU for training, cloud GPUs are a viable alternative. Running this implementation should not cost more than $15.